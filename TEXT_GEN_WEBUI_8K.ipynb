{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManuDash5/Textgen_webui_NEW/blob/main/TEXT_GEN_WEBUI_8K.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generation WebUI 8K\n",
        "\n",
        "This notebook uses https://github.com/oobabooga/text-generation-webui to run whatever model i used in the 3rd cell, not having it today.\n",
        "\n",
        "After running the 3rd cell, a public api link will appear, with that, you can link it to whatever frontend you have.\n",
        "\n",
        "The main feature of this shi- is that after 5 hours in the morning of pure dizzyness i managed to stick a context selector that lets you select what context size you want, supporting models up to 8192 tokens of context, Which the ImBlank2 (thank you bro :3) colab or the \"Colab-TextGen-GPU\" one don't support yet.\n",
        "\n",
        "This thing is as simple as water, don't praise me, i just did the work for you.\n",
        "\n",
        "## Parameters\n",
        "\n",
        "**model:** are you dumb or what? the model availables, if this thing even gets on public eye, MAYBE after a bump on the head i'll add more. Even though you can just add more by inserting more data *and having a brain ðŸ˜€*\n",
        "\n",
        "**context:** the maximum context size in tokens that will be used in whatever frontend you desire, remember to adjust the context size of the frontend to the one that you selected here.\n",
        "\n",
        "## Characters\n",
        "\n",
        "Bro, i am just copying a random description from a notebook that appeared in goddamn https://google.com, i guess these are ways to create your own character, take them in mind.\n",
        "\n",
        "* [JSON character creator](https://oobabooga.github.io/character-creator.html)\n",
        "* I will later add a thing that converts your descriptions to W++, i don't have the link in this moment.\n",
        "\n",
        "#### **Character deposit**\n",
        "\n",
        "If you don't have sillytavern that tells you can download characters from the pyg server and some other silly website, here you have them:\n",
        "\n",
        "* [PygmalionAI Discord](https://discord.gg/pygmalionai)\n",
        "* [Chub.AI](https://chub.ai/) / the silly website i was talkin about ðŸ¥¸"
      ],
      "metadata": {
        "id": "LJCmNckLMznG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "_F76Uuh8etGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuZCqg_0QGtf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 2. Install the web UI\n",
        "#@markdown 1. Stop the cell when given the local link, and go to the next cell.\n",
        "#@markdown 2. This will take a long time (15 minutes+) because i used the one click installer (literally do not mind me, i'm a noob)\n",
        "import os\n",
        "import subprocess\n",
        "import re\n",
        "!sudo apt-get install unzip\n",
        "\n",
        "%cd /content\n",
        "if os.path.exists(\"/content/oobabooga_linux_modified.zip\"):\n",
        "  print(\"Zip already downloaded, skipping.\")\n",
        "else:\n",
        "  !wget https://github.com/ManuDash5/Private-things/raw/main/oobabooga_linux_modified.zip\n",
        "# unzip and install\n",
        "!unzip oobabooga_linux_modified.zip\n",
        "# Auto-select the GPU without input\n",
        "%cd /content/oobabooga_linux\n",
        "!bash start_linux.sh\n",
        "!kill $(ps aux | awk '{print $2}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3. Select, download the model and launch\n",
        "\n",
        "import json\n",
        "import os\n",
        "#@markdown Select the model by the dropdown menu below, if you like this colab i will say, thank you because i know nothing about, not even how i made this.\n",
        "# Parameters\n",
        "model = \"Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\" #@param [\"Pygmalion-13B-SuperHOT-8K-GPTQ\", \"Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\"] {allow-input: false}\n",
        "context = \"4096\" #@param [\"2048\", \"4096\", \"6144\", \"8192\"] {allow-input: false}\n",
        "\n",
        "#@markdown Some models here (Superhot ones) support more than 2k context, make sure to choose the correct context size because colab can run out of memory easily in 6k and 8k.\n",
        "\n",
        "#@markdown If the machine throws an exception like \"Exception: Could not start cloudflared.\" and refuses to give the streaming api link, stop the cell and run it again.\n",
        "# Data\n",
        "models = {\n",
        "    \"Pygmalion-13B-SuperHOT-8K-GPTQ\": (\"TheBloke\", \"Pygmalion-13B-SuperHOT-8K-GPTQ\", \"main\", \"Pygmalion-13B-SuperHOT-8K-GPTQ\"),\n",
        "    \"Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\": (\"TheBloke\", \"Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\", \"main\", \"Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\"),\n",
        "}\n",
        "directory1 = \"/content/oobabooga_linux/text-generation-webui/models/TheBloke_Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\"\n",
        "directory2 = \"/content/oobabooga_linux/text-generation-webui/models/TheBloke_Pygmalion-13B-SuperHOT-8K-GPTQ\"\n",
        "params = set(['--chat'])\n",
        "# Download the model\n",
        "%cd /content/oobabooga_linux/text-generation-webui/\n",
        "huggingface_org, huggingface_repo, huggingface_branch, model_name = models[model]\n",
        "![[ ! -f models/$model_name/config.json ]] && python download-model.py $huggingface_org/$huggingface_repo --branch $huggingface_branch\n",
        "\n",
        "if os.path.exists(directory1):\n",
        "    new_name = \"Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\"\n",
        "else:\n",
        "    print(\"Model directory ready\")\n",
        "\n",
        "if os.path.exists(directory2):\n",
        "    new_name = \"Pygmalion-13B-SuperHOT-8K-GPTQ\"\n",
        "else:\n",
        "    print(\"Model directory ready\")\n",
        "# A extra\n",
        "%cd /content/oobabooga_linux/text-generation-webui/\n",
        "!wget https://oobabooga.github.io/settings-colab.json -O settings-colab-template.json\n",
        "# Launch\n",
        "if context == \"2048\":\n",
        "  params.add('--max_seq_len 2048 --compress_pos_emb 1')\n",
        "if context == \"4096\":\n",
        "  params.add('--max_seq_len 4096 --compress_pos_emb 2')\n",
        "if context == \"6144\":\n",
        "  params.add('--max_seq_len 6144 --compress_pos_emb 3')\n",
        "if context == \"8192\":\n",
        "  params.add('--max_seq_len 8192 --compress_pos_emb 4')\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/oobabooga_linux/text-generation-webui/extensions/api\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/oobabooga_linux/text-generation-webui/\n",
        "cmd = f\"python server.py --load-in-4bit --loader exllama --model {model_name} --settings settings-colab.json {' '.join(params)} --public-api\"\n",
        "print(cmd)\n",
        "!$cmd"
      ],
      "metadata": {
        "id": "h9h3DqArUK_5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}