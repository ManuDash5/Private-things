{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¤¯ Text Generation WebUI 8K ðŸ˜±\n",
        "\n",
        "This notebook uses https://github.com/oobabooga/text-generation-webui to run the models in the 3rd cell.\n",
        "\n",
        "After running the 3rd cell, a public api link will appear, with that, you can copy and link it to whatever frontend you have.\n",
        "\n",
        "The main feature of this colab is that after 5 hours in the morning (12am-5am) i managed to make the ExLLaMa loader work with context sizes bigger than 2048, (see context size selector in the 3rd cell) which the other colabs as i have seen don't support yet.\n",
        "\n",
        "This thing is as simple as water, don't praise me, i just did the work for you.\n",
        "\n",
        "## Parameters\n",
        "\n",
        "**model:** the model availables, if this thing even gets on public eye, MAYBE after a bump on the head i'll add more. Even though you can just add more by inserting more data.\n",
        "\n",
        "**context:** the maximum context size in tokens that will be used in whatever frontend you desire, remember to adjust the context size of the frontend to the one that you selected here.\n",
        "\n",
        "## Characters\n",
        "\n",
        "Bro, i am just copying a random description from a notebook that appeared in goddamn https://google.com, i guess these are ways to create your own character, take them in mind.\n",
        "\n",
        "* [JSON character creator](https://oobabooga.github.io/character-creator.html)\n",
        "* [W++ Text maker](https://nolialsea.github.io/Wpp/) / This site let's you generate W++ descriptions.\n",
        "\n",
        "#### **Character deposit**\n",
        "\n",
        "If you don't have sillytavern that tells you can download characters from the pyg server and some other silly website, here you have them:\n",
        "\n",
        "* [PygmalionAI Discord](https://discord.gg/pygmalionai)\n",
        "* [Chub.AI](https://chub.ai/) / the silly website i was talkin about ðŸ¥¸\n",
        "\n",
        "# Updates\n",
        "#### **Update 1.2** (Normal):\n",
        "\n",
        "* Added new Pygmalion-7b model with 8k context by TheBloke\n",
        "\n",
        "#### **Update 1.1** (Mayor):\n",
        "\n",
        "* Entire overdrawn of the second cell, now instead of using the one time installer, it now uses simple git clone commands for it to be far faster.\n",
        "\n",
        "* Changed the Pygmalion-7b model from TehVenom one to [AnimusOG](https://huggingface.co/AnimusOG/pygmalion-7b-4bit-128g-cuda-2048Token) one.\n",
        "\n",
        "# **Final Note**\n",
        "\n",
        "If you like this colab, i will say, thank you, i started creating this because of a reddit post about ExLLama long context settings, and as no other colab had ExLlama for some reason, this thing happened.\n",
        "\n",
        "I have no programming knowledge and don't even know how i managed to make this work, if an error happens i will bump my head and pray to jesus in the troubleshooting process.\n",
        "\n",
        "Do not take the descriptions too seriously.\n",
        "\n",
        "### Thank you ðŸ¥°"
      ],
      "metadata": {
        "id": "LJCmNckLMznG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "_F76Uuh8etGu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "outputId": "db3fa62d-99ca-4b93-d3dc-d99caafc91d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuZCqg_0QGtf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 2. Install the web UI\n",
        "#@markdown Takes about (or less) than 5 minutes, just go ahead and run the cell, nothing worth mentioning yet.\n",
        "import os\n",
        "\n",
        "# Data\n",
        "directory1 = \"/content/text-generation-webui\"\n",
        "\n",
        "if os.path.exists(directory1):\n",
        "  print(\"You have already installed the web ui.\")\n",
        "else:\n",
        "  !git clone https://github.com/oobabooga/text-generation-webui/\n",
        "  %cd /content/text-generation-webui\n",
        "  !pip install -r requirements.txt\n",
        "\n",
        "!mkdir /content/text-generation-webui/repositories\n",
        "%cd /content/text-generation-webui/repositories\n",
        "!git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\n",
        "!git clone https://github.com/turboderp/exllama.git\n",
        "%cd GPTQ-for-LLaMa\n",
        "!python setup_cuda.py install\n",
        "%cd /content/text-generation-webui/repositories/exllama\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "print(\"###########################################\")\n",
        "print(\"This cell is done, move on to the next one.\")\n",
        "print(\"###########################################\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3. Select, download the model and launch\n",
        "\n",
        "import json\n",
        "import os\n",
        "#@markdown Select the model and other parameters below.\n",
        "# Parameters\n",
        "model = \"Pygmalion-7b (Superhot ver. 8K Max Context)\" #@param [\"Pygmalion-13B (Superhot ver. 8K Max Context)\", \"Wizard-Vicuna-13B-Uncensored (Superhot ver. 8K Max Context)\", \"Pygmalion-7b (2K Max Context)\", \"Metharme-13b (2K Max Context)\", \"Pygmalion-7b (Superhot ver. 8K Max Context)\"] {allow-input: false}\n",
        "context = \"8192\" #@param [\"2048\", \"4096\", \"6144\", \"8192\"] {allow-input: false}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown All the models have in parenthesis their maximum context size, for you to select accordingly, if not, it will throw errors.\n",
        "\n",
        "#@markdown Warning: Colab may not have enough resources for running the models in 8k context, so if you encounter a CUDA or VRAM error, think about lowering the context to 6K or 4K.\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **IMPORTANT:** If the machine throws an exception like \"Exception: Could not start cloudflared.\" and refuses to give the streaming api link, stop the cell and run it again. *WITHOUT* disconnecting the colab entirely.\n",
        "\n",
        "#@markdown **EVEN MORE IMPORTANT:** For the \"CUDA Illegal memory access\" error, do the following: to the selected context size, subtract 100 and put that number in the frontend context size selector instead of just doing the full 6144, because it will error out. (example: 6144-100=6000, 4096-100=3996)\n",
        "# Data\n",
        "models = {\n",
        "    \"Pygmalion-13B (Superhot ver. 8K Max Context)\": (\"TheBloke\", \"Pygmalion-13B-SuperHOT-8K-GPTQ\", \"main\", \"Pygmalion-13B-SuperHOT-8K-GPTQ\"),\n",
        "    \"Wizard-Vicuna-13B-Uncensored (Superhot ver. 8K Max Context)\": (\"TheBloke\", \"Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\", \"main\", \"Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\"),\n",
        "    \"Pygmalion-7b (2K Max Context)\": (\"AnimusOG\", \"pygmalion-7b-4bit-128g-cuda-2048Token\", \"main\", \"pygmalion-7b-4bit-128g-cuda-2048Token\"),\n",
        "    \"Metharme-13b (2K Max Context)\": (\"TehVenom\", \"Metharme-13b-4bit-GPTQ\", \"main\", \"Metharme-13b-4bit-GPTQ\"),\n",
        "    \"Pygmalion-7b (Superhot ver. 8K Max Context)\": (\"TheBloke\", \"Pygmalion-7B-SuperHOT-8K-GPTQ\", \"main\", \"Pygmalion-7B-SuperHOT-8K-GPTQ\"),\n",
        "}\n",
        "directory1 = \"/content/text-generation-webui/models/TheBloke_Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\"\n",
        "directory2 = \"/content/text-generation-webui/models/TheBloke_Pygmalion-13B-SuperHOT-8K-GPTQ\"\n",
        "directory3 = \"/content/text-generation-webui/models/AnimusOG_pygmalion-7b-4bit-128g-cuda-2048Token\"\n",
        "directory4 = \"/content/text-generation-webui/models/TehVenom_Metharme-13b-4bit-GPTQ\"\n",
        "directory5 = \"/content/text-generation-webui/models/TheBloke_Pygmalion-7B-SuperHOT-8K-GPTQ\"\n",
        "params = set(['--chat'])\n",
        "# Download the model\n",
        "%cd /content/text-generation-webui/\n",
        "huggingface_org, huggingface_repo, huggingface_branch, model_name = models[model]\n",
        "![[ ! -f models/$model_name/config.json ]] && python download-model.py $huggingface_org/$huggingface_repo --branch $huggingface_branch\n",
        "\n",
        "%cd /content/text-generation-webui/models/\n",
        "\n",
        "if os.path.exists(directory1):\n",
        "    os.rename(\"TheBloke_Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\", \"Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\")\n",
        "    print(\"Sucessfully changed name\")\n",
        "else:\n",
        "    print(\"Model directory ready\")\n",
        "\n",
        "if os.path.exists(directory2):\n",
        "    os.rename(\"TheBloke_Pygmalion-13B-SuperHOT-8K-GPTQ\", \"Pygmalion-13B-SuperHOT-8K-GPTQ\")\n",
        "    print(\"Sucessfully changed name\")\n",
        "else:\n",
        "    print(\"Model directory ready\")\n",
        "if os.path.exists(directory3):\n",
        "    os.rename(\"AnimusOG_pygmalion-7b-4bit-128g-cuda-2048Token\", \"pygmalion-7b-4bit-128g-cuda-2048Token\")\n",
        "    print(\"Sucessfully changed name\")\n",
        "else:\n",
        "    print(\"Model directory ready\")\n",
        "if os.path.exists(directory4):\n",
        "    os.rename(\"TehVenom_Metharme-13b-4bit-GPTQ\", \"Metharme-13b-4bit-GPTQ\")\n",
        "    print(\"Sucessfully changed name\")\n",
        "else:\n",
        "    print(\"Model directory ready\")\n",
        "if os.path.exists(directory5):\n",
        "    os.rename(\"TheBloke_Pygmalion-7B-SuperHOT-8K-GPTQ\", \"Pygmalion-7B-SuperHOT-8K-GPTQ\")\n",
        "    print(\"Sucessfully changed name\")\n",
        "else:\n",
        "    print(\"Model directory ready\")\n",
        "# A extra\n",
        "%cd /content/text-generation-webui/\n",
        "!wget https://oobabooga.github.io/settings-colab.json -O settings-colab-template.json\n",
        "# Launch\n",
        "if context == \"2048\":\n",
        "  params.add('--max_seq_len 2048 --compress_pos_emb 1')\n",
        "if context == \"4096\":\n",
        "  params.add('--max_seq_len 4096 --compress_pos_emb 2')\n",
        "if context == \"6144\":\n",
        "  params.add('--max_seq_len 6144 --compress_pos_emb 3')\n",
        "if context == \"8192\":\n",
        "  params.add('--max_seq_len 8192 --compress_pos_emb 4')\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/text-generation-webui/extensions/api\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/text-generation-webui/\n",
        "cmd = f\"python server.py --loader exllama --model {model_name} --settings settings-colab.json {' '.join(params)} --load-in-4bit --public-api\"\n",
        "print(cmd)\n",
        "!$cmd"
      ],
      "metadata": {
        "id": "h9h3DqArUK_5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}