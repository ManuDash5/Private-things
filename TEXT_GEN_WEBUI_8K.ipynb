{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¤¯ Text Generation WebUI 8K ðŸ˜±\n",
        "\n",
        "This notebook uses https://github.com/oobabooga/text-generation-webui to run whatever model i used in the 3rd cell, not having it today.\n",
        "\n",
        "After running the 3rd cell, a public api link will appear, with that, you can copy and link it to whatever frontend you have.\n",
        "\n",
        "The main feature of this shi- is that after 5 hours in the morning (12am-5am) of pure dizzyness i managed to stick a context selector that lets you select what context size you want, supporting models up to 8192 tokens of context, Which the ImBlank2 (thank you bro :3) colab or the \"Colab-TextGen-GPU\" one don't support yet.\n",
        "\n",
        "This thing is as simple as water, don't praise me, i just did the work for you.\n",
        "\n",
        "## Parameters\n",
        "\n",
        "**model:** are you dumb or what? the model availables, if this thing even gets on public eye, MAYBE after a bump on the head i'll add more. Even though you can just add more by inserting more data *and having a brain ðŸ˜€*\n",
        "\n",
        "**context:** the maximum context size in tokens that will be used in whatever frontend you desire, remember to adjust the context size of the frontend to the one that you selected here.\n",
        "\n",
        "**Four_bits:** Checkbox to load the model on 4-bits format.\n",
        "\n",
        "**Eight_bits:** Checkbox to load the model on 8-bits format.\n",
        "\n",
        "## Characters\n",
        "\n",
        "Bro, i am just copying a random description from a notebook that appeared in goddamn https://google.com, i guess these are ways to create your own character, take them in mind.\n",
        "\n",
        "* [JSON character creator](https://oobabooga.github.io/character-creator.html)\n",
        "* I will later add a thing that converts your descriptions to W++, i don't have the link in this moment.\n",
        "\n",
        "#### **Character deposit**\n",
        "\n",
        "If you don't have sillytavern that tells you can download characters from the pyg server and some other silly website, here you have them:\n",
        "\n",
        "* [PygmalionAI Discord](https://discord.gg/pygmalionai)\n",
        "* [Chub.AI](https://chub.ai/) / the silly website i was talkin about ðŸ¥¸\n",
        "\n",
        "# **Final Note**\n",
        "\n",
        "If you like this colab, i will say, thank you, i started creating this because of a reddit post about ExLLama long context settings, and as no other colab had ExLlama for some reason, this thing happened.\n",
        "\n",
        "I have no programming knowledge and don't even know how i managed to make this work, if an error happens i will bump my head and pray to jesus in the troubleshooting process.\n",
        "\n",
        "### Thank you ðŸ¥°"
      ],
      "metadata": {
        "id": "LJCmNckLMznG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "_F76Uuh8etGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuZCqg_0QGtf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 2. Install the web UI\n",
        "#@markdown This will take a long time, one of the negative aspects about this colab.\n",
        "\n",
        "#@markdown Approximately 10-15 minutes.\n",
        "import os\n",
        "import subprocess\n",
        "import re\n",
        "!sudo apt-get install unzip\n",
        "# Some data\n",
        "directory1 = \"content/oobabooga_linux_modified.zip\"\n",
        "directory2 = \"content/oobabooga_linux\"\n",
        "# Download the modified zip for this colab\n",
        "%cd /content\n",
        "if os.path.exists(directory1):\n",
        "  print(\"Zip already downloaded, skipping.\")\n",
        "else:\n",
        "  !wget https://github.com/ManuDash5/Textgen_webui_NEW/raw/main/oobabooga_linux_modified.zip\n",
        "# unzip and install\n",
        "if os.path.exists(directory2):\n",
        "  print(\"Already unzipped, skipping.\")\n",
        "else:\n",
        "  !unzip oobabooga_linux_modified.zip\n",
        "# Starting\n",
        "%cd /content/oobabooga_linux\n",
        "!bash start_linux.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3. Select, download the model and launch\n",
        "\n",
        "import json\n",
        "import os\n",
        "#@markdown Select the model and other parameters below.\n",
        "# Parameters\n",
        "model = \"Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\" #@param [\"Pygmalion-13B-SuperHOT-8K-GPTQ\", \"Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\", \"Pygmalion-7b-4bit-GPTQ-Safetensors\", \"Metharme-13b-4bit-GPTQ\"] {allow-input: false}\n",
        "context = \"6144\" #@param [\"2048\", \"4096\", \"6144\", \"8192\"] {allow-input: false}\n",
        "Four_bits = True #@param {type:\"boolean\"}\n",
        "Eight_bits = False #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown Pygmalion 13B and Wizard-Vicuna 13B are in 4 bits, so you know, to check the box.\n",
        "\n",
        "#@markdown ONLY the following models support more than 2k context, make sure to choose the correct context size because colab can run out of memory easily in 6k and 8k, even in 4k.\n",
        "\n",
        "#@markdown * Pygmalion 13B\n",
        "#@markdown * Wizard-Vicuna 13B\n",
        "#@markdown ---\n",
        "#@markdown If the machine throws an exception like \"Exception: Could not start cloudflared.\" and refuses to give the streaming api link, stop the cell and run it again.\n",
        "\n",
        "# Data\n",
        "models = {\n",
        "    \"Pygmalion-13B-SuperHOT-8K-GPTQ\": (\"TheBloke\", \"Pygmalion-13B-SuperHOT-8K-GPTQ\", \"main\", \"Pygmalion-13B-SuperHOT-8K-GPTQ\"),\n",
        "    \"Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\": (\"TheBloke\", \"Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\", \"main\", \"Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\"),\n",
        "    \"Pygmalion-7b-4bit-GPTQ-Safetensors\": (\"TehVenom\", \"Pygmalion-7b-4bit-GPTQ-Safetensors\", \"main\", \"Pygmalion-7b-4bit-GPTQ-Safetensors\"),\n",
        "    \"Metharme-13b-4bit-GPTQ\": (\"TehVenom\", \"Metharme-13b-4bit-GPTQ\", \"main\", \"Metharme-13b-4bit-GPTQ\"),\n",
        "}\n",
        "directory1 = \"/content/oobabooga_linux/text-generation-webui/models/TheBloke_Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\"\n",
        "directory2 = \"/content/oobabooga_linux/text-generation-webui/models/TheBloke_Pygmalion-13B-SuperHOT-8K-GPTQ\"\n",
        "directory3 = \"/content/oobabooga_linux/text-generation-webui/models/TehVenom_Pygmalion-7b-4bit-GPTQ-Safetensors\"\n",
        "directory4 = \"/content/oobabooga_linux/text-generation-webui/models/TehVenom_Metharme-13b-4bit-GPTQ\"\n",
        "params = set(['--chat'])\n",
        "# Download the model\n",
        "%cd /content/oobabooga_linux/text-generation-webui/\n",
        "huggingface_org, huggingface_repo, huggingface_branch, model_name = models[model]\n",
        "![[ ! -f models/$model_name/config.json ]] && python download-model.py $huggingface_org/$huggingface_repo --branch $huggingface_branch\n",
        "\n",
        "%cd /content/oobabooga_linux/text-generation-webui/models/\n",
        "\n",
        "if os.path.exists(directory1):\n",
        "    os.rename(\"TheBloke_Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\", \"Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ\")\n",
        "    print(\"Sucessfully changed name\")\n",
        "else:\n",
        "    print(\"Model directory ready\")\n",
        "\n",
        "if os.path.exists(directory2):\n",
        "    os.rename(\"TheBloke_Pygmalion-13B-SuperHOT-8K-GPTQ\", \"Pygmalion-13B-SuperHOT-8K-GPTQ\")\n",
        "    print(\"Sucessfully changed name\")\n",
        "else:\n",
        "    print(\"Model directory ready\")\n",
        "if os.path.exists(directory3):\n",
        "    os.rename(\"TehVenom_Pygmalion-7b-4bit-GPTQ-Safetensors\", \"Pygmalion-7b-4bit-GPTQ-Safetensors\")\n",
        "    print(\"Sucessfully changed name\")\n",
        "else:\n",
        "    print(\"Model directory ready\")\n",
        "if os.path.exists(directory4):\n",
        "    os.rename(\"TehVenom_Metharme-13b-4bit-GPTQ\", \"Metharme-13b-4bit-GPTQ\")\n",
        "    print(\"Sucessfully changed name\")\n",
        "else:\n",
        "    print(\"Model directory ready\")\n",
        "# A extra\n",
        "%cd /content/oobabooga_linux/text-generation-webui/\n",
        "!wget https://oobabooga.github.io/settings-colab.json -O settings-colab-template.json\n",
        "# Launch\n",
        "if context == \"2048\":\n",
        "  params.add('--max_seq_len 2048 --compress_pos_emb 1')\n",
        "if context == \"4096\":\n",
        "  params.add('--max_seq_len 4096 --compress_pos_emb 2')\n",
        "if context == \"6144\":\n",
        "  params.add('--max_seq_len 6144 --compress_pos_emb 3')\n",
        "if context == \"8192\":\n",
        "  params.add('--max_seq_len 8192 --compress_pos_emb 4')\n",
        "if Four_bits == True:\n",
        "  params.add('--load-in-4bit')\n",
        "if Eight_bits == True:\n",
        "  params.add('--load-in-8bit')\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/oobabooga_linux/text-generation-webui/extensions/api\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/oobabooga_linux/text-generation-webui/\n",
        "cmd = f\"python server.py --loader exllama --model {model_name} --settings settings-colab.json {' '.join(params)} --public-api\"\n",
        "print(cmd)\n",
        "!$cmd"
      ],
      "metadata": {
        "id": "h9h3DqArUK_5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}